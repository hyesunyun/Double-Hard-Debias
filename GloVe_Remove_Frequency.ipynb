{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation](https://arxiv.org/abs/2005.00965)\n",
    "\n",
    "For more detailed explanations, please refer to the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load original embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs, os, json, operator, pickle\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import scipy\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322636 (322636, 300) 322636\n"
     ]
    }
   ],
   "source": [
    "def load_glove(path):\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    wv = []\n",
    "    vocab = []\n",
    "    for line in lines:\n",
    "        tokens = line.strip().split(\" \")\n",
    "        assert len(tokens) == 301\n",
    "        vocab.append(tokens[0])\n",
    "        wv.append([float(elem) for elem in tokens[1:]])\n",
    "    w2i = {w: i for i, w in enumerate(vocab)}\n",
    "    wv = np.array(wv).astype(float)\n",
    "    print(len(vocab), wv.shape, len(w2i))\n",
    "    \n",
    "    return wv, w2i, vocab\n",
    "\n",
    "wv, w2i, vocab = load_glove('./data/vectors.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute original bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_embed = wv[w2i['he'], :]\n",
    "she_embed = wv[w2i['she'], :]\n",
    "\n",
    "def simi(a, b):\n",
    "    return 1-scipy.spatial.distance.cosine(a, b)\n",
    "\n",
    "def compute_bias_by_projection(wv, w2i, vocab):\n",
    "    d = {}\n",
    "    for w in vocab:\n",
    "        u = wv[w2i[w], :]\n",
    "        d[w] = simi(u, he_embed) - simi(u, she_embed)\n",
    "    return d\n",
    "\n",
    "gender_bias_bef = compute_bias_by_projection(wv_limit, w2i_limit, vocab_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Frequency Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# get main PCA components\n",
    "def my_pca(wv):\n",
    "    wv_mean = np.mean(np.array(wv), axis=0)\n",
    "    wv_hat = np.zeros(wv.shape).astype(float)\n",
    "\n",
    "    for i in range(len(wv)):\n",
    "        wv_hat[i, :] = wv[i, :] - wv_mean\n",
    "\n",
    "    main_pca = PCA()\n",
    "    main_pca.fit(wv_hat)\n",
    "    \n",
    "    return main_pca\n",
    "\n",
    "main_pca = my_pca(wv)\n",
    "wv_mean = np.mean(np.array(wv), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_frequency(wv, w2i, w2i_partial, vocab_partial, component_ids):\n",
    "    \n",
    "    D = []\n",
    "\n",
    "    for i in component_ids:\n",
    "        D.append(main_pca.components_[i])\n",
    "    \n",
    "    # get rid of frequency features\n",
    "    wv_f = np.zeros((len(vocab_partial), wv.shape[1])).astype(float)\n",
    "    \n",
    "    for i, w in enumerate(vocab_partial):\n",
    "        u = wv[w2i[w], :]\n",
    "        sub = np.zeros(u.shape).astype(float)\n",
    "        for d in D:\n",
    "            sub += np.dot(np.dot(np.transpose(d), u), d)\n",
    "        wv_f[w2i_partial[w], :] = wv[w2i[w], :] - sub - wv_mean\n",
    "    \n",
    "    print(wv_f.shape)\n",
    "    return wv_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "component id:  1\n",
      "(322636, 300)\n"
     ]
    }
   ],
   "source": [
    "#Remove second component because authors found best performance \n",
    "#when the second component was removed.\n",
    "component_id=1 \n",
    "wv_f = remove_frequency(wv, w2i, w2i_partial = w2i, vocab_partial = vocab, component_ids = [component_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/vectors_frequency_removed.txt\", \"w\") as outputFile:\n",
    "    for i in range(len(vocab)):\n",
    "        word = vocab[i]\n",
    "        embedding = word + \" \" + \" \".join([str(feature) for feature in wv_f[w2i[word],:]])\n",
    "        outputFile.write(embedding + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
